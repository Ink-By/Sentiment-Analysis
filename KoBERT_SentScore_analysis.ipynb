{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDSpHB9xowL3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "bY2W-JYPo1QM",
        "outputId": "f5f78f14-185f-47c8-8bd1-f2038009144c"
      },
      "outputs": [
        {
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsuoaT5rpA2D"
      },
      "outputs": [],
      "source": [
        "  # Input CSVs on Google Drive\n",
        "  # /content/drive/MyDrive/csvs/4차년도.csv\n",
        "  # /content/drive/MyDrive/csvs/5차년도.csv\n",
        "  # /content/drive/MyDrive/csvs/5차년도_2차.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yyj_3VepezD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_paths = [\n",
        "    '/content/drive/MyDrive/csvs/4차년도.csv',\n",
        "    '/content/drive/MyDrive/csvs/5차년도.csv',\n",
        "    '/content/drive/MyDrive/csvs/5차년도_2차.csv'\n",
        "]\n",
        "\n",
        "dataframes = [pd.read_csv(file, encoding='cp949') for file in file_paths]\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "duplicate_rows = combined_df[combined_df.duplicated()]\n",
        "\n",
        "cleaned_df = combined_df.drop_duplicates()\n",
        "\n",
        "print(\"Duplicate rows found:\" if not duplicate_rows.empty else \"No duplicate rows found.\")\n",
        "print(duplicate_rows)\n",
        "\n",
        "cleaned_df.to_csv('/content/drive/MyDrive/csvs/cleaned_combined.csv', index=False, encoding='cp949')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wu2Intlk8yqa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.bool = np.bool_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvFoxi_VS84K"
      },
      "outputs": [],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIw3Tkuu5ptf"
      },
      "outputs": [],
      "source": [
        "# boto3 <=1.15.18\n",
        "# gluonnlp >= 0.6.0, <=0.10.0\n",
        "# mxnet >= 1.4.0, <=1.7.0.post2\n",
        "# onnxruntime == 1.8.0, <=1.8.0\n",
        "# sentencepiece >= 0.1.6, <=0.1.96\n",
        "# torch >= 1.7.0, <=1.10.1\n",
        "# transformers >= 4.8.1, <=4.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgPhxCRHURE7"
      },
      "outputs": [],
      "source": [
        "!pip install gluonnlp pandas tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnjccgQkVGMA"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEKLRdgBVCim"
      },
      "outputs": [],
      "source": [
        "!pip install mxnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N3e3n9hVDjv"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece==0.1.96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jxQZOIiVEtE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JB5U_4_jJ-7Z"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers==4.8.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Bqw3eA31Qo"
      },
      "outputs": [],
      "source": [
        "# !pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDQoY_sWCGS8"
      },
      "outputs": [],
      "source": [
        "# https://github.com/SKTBrain/KoBERT 의 파일들을 Colab으로 다운로드\n",
        "!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master --no-deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnpwc5fHPI4T"
      },
      "outputs": [],
      "source": [
        "!pip install gluonnlp pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xGwqjga72sL"
      },
      "outputs": [],
      "source": [
        "# !pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdLO1IsI8DUX"
      },
      "outputs": [],
      "source": [
        "# pip install boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIPnyrtc8aQB"
      },
      "outputs": [],
      "source": [
        "# !pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbY1k3VQ3f-v"
      },
      "source": [
        "# koBERT\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMBq7AmJW1VP"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BAVFFKtWtHv"
      },
      "outputs": [],
      "source": [
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDoNp_pD3iVX"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XVOFsNn3lwt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pyi9SWg83n4k"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XM3vofUh-d6s"
      },
      "outputs": [],
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvXC_b1i-gmo"
      },
      "outputs": [],
      "source": [
        "# [AI Hub] Conversation speech dataset for sentiment classification\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/csvs/cleaned_combined.csv\", encoding='cp949')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ToHWQmH-nWn"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq9vcwzV-s6Z"
      },
      "outputs": [],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msHBXap9-2-T"
      },
      "outputs": [],
      "source": [
        "data['상황'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpu4tKTW-5Uh"
      },
      "outputs": [],
      "source": [
        "# Map 7 emotion classes to numeric labels\n",
        "# 0: negative (fear), 1: neutral (surprise), 2: negative (angry/anger),\n",
        "# 3: negative (sadness/sad), 4: neutral (neutral), 5: positive (happiness), 6: negative (disgust)\n",
        "data.loc[(data['상황'] == \"fear\"), '상황'] = 0\n",
        "data.loc[(data['상황'] == \"surprise\"), '상황'] = 1\n",
        "data.loc[(data['상황'] == \"angry\"), '상황'] = 2\n",
        "data.loc[(data['상황'] == \"anger\"), '상황'] = 2\n",
        "data.loc[(data['상황'] == \"sadness\"), '상황'] = 3\n",
        "data.loc[(data['상황'] == \"neutral\"), '상황'] = 4\n",
        "data.loc[(data['상황'] == \"happiness\"), '상황'] = 5\n",
        "data.loc[(data['상황'] == \"disgust\"), '상황'] = 6\n",
        "data.loc[(data['상황'] == \"sad\"), '상황'] = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-R2snW-9_BpK"
      },
      "outputs": [],
      "source": [
        "data['상황'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7AOS_O5_F5g"
      },
      "outputs": [],
      "source": [
        "data_list = []\n",
        "for ques, label in zip (data['발화문'], data['상황']):\n",
        "  data = []\n",
        "  data.append(ques)\n",
        "  data.append(str(label))\n",
        "\n",
        "  data_list.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npyHtUZX_xs4"
      },
      "outputs": [],
      "source": [
        "print(data)\n",
        "print(data_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UfeaYfn_1AY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ1sC9Fe_18s"
      },
      "outputs": [],
      "source": [
        "print(len(dataset_train), len(dataset_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7c6zhkM_3u4"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR7W-ZZC_6jX"
      },
      "outputs": [],
      "source": [
        "# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수\n",
        "# 출처 : https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len, pad, pair):\n",
        "\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len,vocab = vocab, pad = pad, pair = pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HX4vhlW_8Gf"
      },
      "outputs": [],
      "source": [
        "# parameter 값 출처 : https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 10\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a0jilP0Ii9E"
      },
      "outputs": [],
      "source": [
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCHhdejB__Z2"
      },
      "outputs": [],
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tok, vocab, max_len, False, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tok, vocab, max_len, False, False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IAC6sxqALMS"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, num_workers = 0)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, num_workers = 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJP7fjW1ANaU"
      },
      "source": [
        "### KoBERT 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb7rjdc3ALn4"
      },
      "outputs": [],
      "source": [
        "# KoBERT 오픈소스 내 예제코드 : https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes = 7,   # 감정 클래스 수로 조정\n",
        "                 dr_rate = None,\n",
        "                 params = None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p = dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict = False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx5E7htgYyxT"
      },
      "outputs": [],
      "source": [
        "model = BERTClassifier(bertmodel,  dr_rate = 0.5).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN7WgRvkY2qo"
      },
      "outputs": [],
      "source": [
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr = learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_step, num_training_steps = t_total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdDpmxn-Y6Dx"
      },
      "outputs": [],
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AaKanwybCFt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    try:\n",
        "        token_ids = pad_sequence([torch.tensor(item[0]) for item in batch], batch_first=True, padding_value=0)\n",
        "        valid_length = torch.tensor([int(item[1]) for item in batch])\n",
        "        segment_ids = pad_sequence([torch.tensor(item[2]) for item in batch], batch_first=True, padding_value=0)\n",
        "        label = torch.tensor([int(item[3]) for item in batch])\n",
        "    except Exception as e:\n",
        "        print(\"Data Error:\", e)\n",
        "        print(\"Batch Content:\", batch)\n",
        "        raise e\n",
        "\n",
        "    return token_ids, valid_length, segment_ids, label\n",
        "\n",
        "train_dataloader = DataLoader(data_train, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ_ZElMj16EF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVydvXhIDczT"
      },
      "source": [
        "### 각 감성 분석 및 가중치 부여"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aX7uy3hLx94N"
      },
      "outputs": [],
      "source": [
        "!pip install soynlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBIYNHCbzmJ_"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git\n",
        "!cd Mecab-ko-for-Google-Colab\n",
        "!bash install_mecab-ko_on_colab_light_220429.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCi_WuM1yvpH"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMAF2m-EzIJd"
      },
      "outputs": [],
      "source": [
        "!cd Mecab-ko-for-Google-Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo_9zlWazNJq"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoGvsLvgzLe3"
      },
      "outputs": [],
      "source": [
        "!bash install_mecab-ko_on_colab_light_220429.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGxb7VwMDyOa"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from konlpy.tag import Mecab\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mecab = Mecab(dicpath='/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
        "\n",
        "def load_sentiword_dict(filepath):\n",
        "    sentiword_dict = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[0]\n",
        "                try:\n",
        "                    score = int(parts[1])\n",
        "                    sentiword_dict[word] = score\n",
        "                except ValueError:\n",
        "                    pass\n",
        "    return sentiword_dict\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    normalized_sent = repeat_normalize(text, num_repeats=2)\n",
        "    return normalized_sent\n",
        "\n",
        "def calculate_sentiment_score(text, sentiword_dict):\n",
        "    tokens = mecab.morphs(text)\n",
        "    score = 0\n",
        "    token_scores = []\n",
        "    for token in tokens:\n",
        "        if token in sentiword_dict:\n",
        "            token_score = sentiword_dict[token]\n",
        "            score += token_score\n",
        "            token_scores.append((token, token_score))\n",
        "        else:\n",
        "            token_scores.append((token, 0))\n",
        "    return score, token_scores\n",
        "\n",
        "filepath = '/content/drive/MyDrive/grad_model_kobert/SentiWord_Dict.txt'\n",
        "sentiword_dict = load_sentiword_dict(filepath)\n",
        "\n",
        "folder_path = '/content/drive/MyDrive/grad_model_kobert/dataset_3month'\n",
        "output_folder = '/content/drive/MyDrive/grad_model_kobert/sent-result-3month'  \n",
        "\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.xlsx'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        \n",
        "        review_scores = []\n",
        "        for idx, row in df.iterrows():\n",
        "            review = row['content']\n",
        "            preprocessed_review = preprocess_text(review)\n",
        "            score, token_scores = calculate_sentiment_score(preprocessed_review, sentiword_dict)\n",
        "\n",
        "            \n",
        "            star_weight = row['score'] / 5.0  \n",
        "            thumbs_weight = row['thumbsUpCount'] / 100.0  \n",
        "            if row['thumbsUpCount'] == 0:\n",
        "                thumbs_weight = 1\n",
        "\n",
        "            weighted_score = score * star_weight * thumbs_weight\n",
        "            senti = 0\n",
        "            if weighted_score > 0:\n",
        "                senti = 1\n",
        "            elif weighted_score < 0:\n",
        "                senti = -1\n",
        "\n",
        "            star_senti = 0\n",
        "            if row['score'] <= 2:\n",
        "                star_senti = -1\n",
        "            elif row['score'] >= 4:\n",
        "                star_senti = 1\n",
        "\n",
        "            review_scores.append((senti, star_senti))\n",
        "\n",
        "        \n",
        "        df['senti'] = [s[0] for s in review_scores]\n",
        "        df['star_senti'] = [s[1] for s in review_scores]\n",
        "\n",
        "        \n",
        "        new_filename = f\"3month_senti_{filename}\"\n",
        "        output_path = os.path.join(output_folder, new_filename)\n",
        "\n",
        "        \n",
        "        df.to_excel(output_path, index=False)\n",
        "        print(f\"Sentiment analysis completed for {filename}. Results saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVirDpIKunBA"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZVjhIMIumyw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from konlpy.tag import Mecab\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "mecab = Mecab(dicpath='/usr/local/lib/mecab/dic/mecab-ko-dic')\n",
        "\n",
        "\n",
        "def load_sentiword_dict(filepath):\n",
        "    sentiword_dict = {}\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) >= 2:\n",
        "                word = parts[0]\n",
        "                try:\n",
        "                    score = int(parts[1])\n",
        "                    sentiword_dict[word] = score\n",
        "                except ValueError:\n",
        "                    pass\n",
        "    return sentiword_dict\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    normalized_sent = repeat_normalize(text, num_repeats=2)\n",
        "    return normalized_sent\n",
        "filepath = '/content/drive/MyDrive/grad_model_kobert/SentiWord_Dict.txt'\n",
        "sentiword_dict = load_sentiword_dict(filepath)\n",
        "\n",
        "\n",
        "def calculate_sentiment_score(text, sentiword_dict):\n",
        "    tokens = mecab.morphs(text)\n",
        "    score = 0\n",
        "    token_scores = []\n",
        "    for token in tokens:\n",
        "        if token in sentiword_dict:\n",
        "            token_score = sentiword_dict[token]\n",
        "            score += token_score\n",
        "            token_scores.append((token, token_score))\n",
        "        else:\n",
        "            token_scores.append((token, 0))\n",
        "    return score, token_scores\n",
        "\n",
        "\n",
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, dr_rate=0.5):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        _, pooler = self.bert(input_ids=token_ids, token_type_ids=segment_ids)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)\n",
        "\n",
        "\n",
        "model = BERTClassifier(bertmodel).to('cuda')\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/grad_model_kobert/model.pth'))\n",
        "model.eval()\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "\n",
        "def predict_sentiment_kobert(text):\n",
        "    max_len = 64\n",
        "    tokenized_text = tok(text)\n",
        "    token_ids = torch.tensor([vocab[token] for token in tokenized_text]).unsqueeze(0).to('cuda')\n",
        "    segment_ids = torch.zeros_like(token_ids).to('cuda')\n",
        "    valid_length = torch.tensor([min(len(tokenized_text), max_len)]).to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(token_ids, valid_length, segment_ids)\n",
        "        score = output.argmax().item()\n",
        "\n",
        "    if score == 0:\n",
        "        return -1  \n",
        "    elif score == 1:\n",
        "        return 0  \n",
        "    else:\n",
        "        return 1  \n",
        "\n",
        "\n",
        "folder_path = './content/drive/MyDrive/grad_model_kobert/dataset_3month'\n",
        "output_folder = '/content/drive/MyDrive/grad_model_kobert/sent-result-3month'\n",
        "\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.xlsx'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        df = pd.read_excel(file_path)\n",
        "\n",
        "        review_scores = []\n",
        "        senti_dict_scores = []\n",
        "        for idx, row in df.iterrows():\n",
        "            review = row['content']\n",
        "            preprocessed_review = preprocess_text(review)\n",
        "\n",
        "            \n",
        "            senti_dict_score, _ = calculate_sentiment_score(preprocessed_review, sentiword_dict)\n",
        "            senti_dict_scores.append(senti_dict_score)\n",
        "\n",
        "            \n",
        "            senti_kobert = predict_sentiment_kobert(preprocessed_review)\n",
        "\n",
        "            \n",
        "            star_weight = row['score'] / 5.0\n",
        "            thumbs_weight = row['thumbsUpCount'] / 100.0\n",
        "            if row['thumbsUpCount'] == 0:\n",
        "                thumbs_weight = 1\n",
        "\n",
        "            \n",
        "            combined_score = (senti_kobert + senti_dict_score) / 2  \n",
        "            weighted_score = combined_score * star_weight * thumbs_weight\n",
        "            review_scores.append(weighted_score)\n",
        "\n",
        "        \n",
        "        df['senti_dict_score'] = senti_dict_scores\n",
        "        df['weighted_senti'] = review_scores\n",
        "        new_filename = f\"3month_senti_{filename}\"\n",
        "        output_path = os.path.join(output_folder, new_filename)\n",
        "        df.to_excel(output_path, index=False)\n",
        "        print(f\"Sentiment analysis completed for {filename}. Results saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g0MrVJUEIWB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, dr_rate=0.5):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "        self.classifier = nn.Linear(768, 7) \n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        _, pooler = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
        "        if self.dr_rate:\n",
        "            out = F.dropout(pooler, p=self.dr_rate, training=self.training)\n",
        "        return self.classifier(out)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = '/content/drive/MyDrive/grad_model_kobert/model.pth'\n",
        "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
        "\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, reviews, tokenizer, max_len=64):\n",
        "        self.reviews = reviews\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review = str(self.reviews[idx])\n",
        "        encoding = self.tokenizer(review, max_length=self.max_len, padding='max_length', truncation=True)\n",
        "        input_ids = torch.tensor(encoding['input_ids'])\n",
        "        attention_mask = torch.tensor(encoding['attention_mask'])\n",
        "        token_type_ids = torch.tensor(encoding['token_type_ids'])\n",
        "        return input_ids, attention_mask, token_type_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "def classify_and_save(folder_path, output_folder):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.xlsx'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            df = pd.read_excel(file_path)\n",
        "            reviews = df['content'].fillna('').tolist()\n",
        "\n",
        "            dataset = ReviewDataset(reviews, tok, max_len=64)\n",
        "            dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "            results = []\n",
        "            with torch.no_grad():\n",
        "                for input_ids, attention_mask, token_type_ids in tqdm(dataloader):\n",
        "                    input_ids = input_ids.to(device)\n",
        "                    attention_mask = attention_mask.to(device)\n",
        "                    token_type_ids = token_type_ids.to(device)\n",
        "\n",
        "                    outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "                    _, preds = torch.max(outputs, dim=1)\n",
        "                    results.extend(preds.cpu().numpy())\n",
        "\n",
        "        \n",
        "            df['emotion'] = results\n",
        "            new_filename = f\"3month_emotion_{filename}\"\n",
        "            output_path = os.path.join(output_folder, new_filename)\n",
        "            df.to_excel(output_path, index=False)\n",
        "            print(f\"Emotion classification completed for {filename}. Results saved to {output_path}\")\n",
        "\n",
        "folder_path = '/content/datasets/review_3m' \n",
        "output_folder = '/content/senti-result-감성사전-3m' \n",
        "classify_and_save(folder_path, output_folder)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
